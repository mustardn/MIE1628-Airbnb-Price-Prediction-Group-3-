{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spark imports\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql import Row, SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, split, mean, count, isnan, when, udf, abs, sqrt, max\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.ml.feature import CountVectorizer, VectorIndexer, VectorAssembler,StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline, feature\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "# Import AirBnB dataset\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.sql(\"SELECT * FROM listings_csv\")\n",
    "\n",
    "#Drop rows with data quality issues - Hide if issues no longer exist in data import\n",
    "#df = df.filter(col('property_type').isin(['Apartment', 'House', 'Condominium', 'Townhouse', 'Guest Suite', 'Bungalow', 'Loft', 'Serviced apartment', 'Bed and breakfast', 'Guesthouse', 'Villa', 'Boutique hotel', 'Hostel' ])) \n",
    "\n",
    "# Specify list of columns that need to be deleted as they won't impact the price to be predicted\n",
    "columns_to_drop = ['id','listing_url', 'scrape_id', 'last_scraped','name', 'summary', 'space', 'description', 'experiences_offered',\n",
    "                  'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'thumbnail_url','medium_url', 'picture_url',\n",
    "                  'xl_picture_url', 'host_id', 'host_url', 'host_name', 'host_since', 'host_location', 'square_feet', 'host_about', 'host_response_time',\n",
    "                  'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_thumbnail_url', 'host_picture_url', 'host_listings_count',\n",
    "                  'host_total_listings_count','host_verifications', 'host_has_profile_pic', 'host_identity_verified','street', 'neighbourhood_group_cleansed',\n",
    "                  'city','state','market', 'smart_location','country_code','country','latitude', 'longitude', 'is_location_exact','weekly_price',\n",
    "                  'monthly_price','minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "                  'maximum_maximum_nights', 'calendar_updated', 'has_availability','availability_30','availability_60','availability_90', \n",
    "                  'availability_365','calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'first_review', 'last_review',\n",
    "                  'review_scores_rating','review_scores_accuracy','review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n",
    "                  'review_Scores_location', 'review_Scores_value', 'license', 'jurisdiction_names',  'requires_guest_profile',\n",
    "                  'requires_guest_phone_verification','calculated_host_listings_count', 'calculated_host_listings_count_entire_homes','calculated_host_listings_count_private_rooms',\n",
    "                  'calculated_host_listings_count_shared_rooms','reviews_per_month', 'host_neighbourhood', 'neighbourhood', 'zipcode','requires_license',\n",
    "                   'is_business_travel_ready', 'require_guest_profile_picture', 'require_guest_phone_verification', 'extra_people', 'minimum_nights_avg_ntm','maximum_nights_avg_ntm']\n",
    "\n",
    "#Drop columns listed above\n",
    "data = df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Import Neighbourhood Dataset ie Wellbeing dataset with education and econominc indicators of neighbourhood population\n",
    "neighbourhood_df = sqlContext.sql(\"SELECT * FROM wellbeing_toronto_csv\").drop(\"Combined_Indicators\", \"Neighbourhood_ID\")\n",
    "\n",
    "# join the listings with the wellbeing neighbourhood data by neighbourhood name\n",
    "airbnb_df = data.join(neighbourhood_df, data.neighbourhood_cleansed == neighbourhood_df.Neighbourhood).drop(\"Neighbourhood\", \"Neighbourhood_ID\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Import MCI data set and find average number of MCI events occurring in the neighbourhood and obtain ranks (1 - lowest occurances, max - highest occurances)\n",
    "mci_df = sqlContext.sql(\"SELECT mci_2014_to_2018_csv.occurrenceyear, mci_2014_to_2018_csv.MCI, LEFT(mci_2014_to_2018_csv.Neighbourhood,POSITION(' (' IN mci_2014_to_2018_csv.Neighbourhood)-1) AS cleaned_Neighbourhood FROM mci_2014_to_2018_csv WHERE mci_2014_to_2018_csv.occurrenceyear >=2014\")\n",
    "mci_df = mci_df.groupBy([\"cleaned_Neighbourhood\",\"occurrenceyear\"]).agg(count('occurrenceyear')).orderBy([\"cleaned_Neighbourhood\",\"occurrenceyear\"], ascending = True)\n",
    "mci_df = mci_df.groupby([\"cleaned_Neighbourhood\"]).agg(mean('count(occurrenceyear)')).orderBy(\"avg(count(occurrenceyear))\", ascending = False)\n",
    "mci_df = mci_df.withColumn(\"rank\",dense_rank().over(Window.orderBy(\"avg(count(occurrenceyear))\")))\n",
    "mci_df = mci_df.withColumn(\"cleaned_Neighbourhood\", regexp_replace('cleaned_Neighbourhood','_','-'))\n",
    "max_rank = mci_df.agg(max('rank'))\n",
    "# join the listings with the MCI data by neighbourhood name\n",
    "airbnb_df = airbnb_df.join(mci_df, airbnb_df.neighbourhood_cleansed == mci_df.cleaned_Neighbourhood).drop('cleaned_Neighbourhood','avg(count(occurrenceyear))')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Check if data in cell, if no data present return 0 else 1\n",
    "def true_false_categorization(text):\n",
    "  if text != None:\n",
    "    return (1)\n",
    "  else:\n",
    "    return(0)\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  # Recode cells with true or false values to be 1 or 0\n",
    "def binary_encoding(text):\n",
    "  if 'f' in text:\n",
    "    return (0)\n",
    "  else:\n",
    "    return (1)\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  # Replace a characther in a string \n",
    "def replace_char(df,column_name,orig_char, new_char):\n",
    "  return(df.withColumn(column_name, regexp_replace(column_name,orig_char,new_char)))\n",
    "\n",
    "\n",
    "\n",
    "# Convert data in column to an array so as to use with Count Vectorizer\n",
    "def convert_col_to_array(df,new_col,orig_col, split_char):\n",
    "  return(df.withColumn(new_col,split(col(orig_col),split_char)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize_Count_Vectorizer(input_Col, output_Col):\n",
    "  return(CountVectorizer(inputCol = input_Col, outputCol = output_Col))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_dtype(df, colname,new_dtype):\n",
    "  return(df.withColumn(colname, df[colname].cast(new_dtype)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def categorize_ranks(rank):\n",
    "  max_rank = 137\n",
    "  if rank > 0 * max_rank and rank <=0.25 * max_rank:\n",
    "    return(\"No-Low\")\n",
    "  elif rank > 0.25*max_rank and rank <=0.5 *max_rank:\n",
    "    return(\"Low-Med\")\n",
    "  elif rank >0.5 * max_rank and rank <= 0.75*max_rank:\n",
    "    return (\"Med-High\")\n",
    "  else:\n",
    "    return (\"High\")\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  # Convert True False values into boolean values\n",
    "udf_binary_encoding = udf(binary_encoding)\n",
    "airbnb_df = airbnb_df.withColumn('Instant_Bookable',udf_binary_encoding('instant_bookable'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# String Hashing on Amenities\n",
    "tokenizer = RegexTokenizer(inputCol=\"amenities\", outputCol=\"amenitieslist\", pattern=\"\\\\W+\")\n",
    "tokenized = tokenizer.transform(airbnb_df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"amenitiesFeatures\", numFeatures=40)\n",
    "airbnb_df = hashingTF.transform(tokenized).drop(\"amenities\", \"amenitieslist\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "udf_recode_true_false = udf(true_false_categorization)\n",
    "airbnb_df = airbnb_df.withColumn('house_rules', udf_recode_true_false('house_rules')) #Are there any house rules?\n",
    "airbnb_df = airbnb_df.withColumn('security_deposit', udf_recode_true_false('security_deposit')) # Is a security deposit required\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Drop NAs from Property Type and Bed Type ie these must exist in the listing to be considered\n",
    "list = ['property_type', 'bed_type']\n",
    "airbnb_df = airbnb_df.dropna(subset=list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "udf_rank_categorization = udf(categorize_ranks)\n",
    "#airbnb_df = airbnb_df.withColumn('rank', udf_rank_categorization('rank')) # Is a security deposit required\n",
    "airbnb_df = airbnb_df.withColumn('rank', udf_rank_categorization('rank'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#One Hot Encoding\n",
    "\n",
    "OHE_List= ['property_type', 'room_type', 'bed_type', 'neighbourhood_cleansed', 'cancellation_policy', 'rank']\n",
    "\n",
    "# Generate OHE for elements in the list above\n",
    "for element in OHE_List:\n",
    "  #Remove Spaces from categorical fields to prevent issues with encoding\n",
    "  airbnb_df = replace_char(airbnb_df, element,\" \", \"_\")\n",
    "  \n",
    "  #Convert String to Array so that OHE can run (requires an array as an input)\n",
    "  airbnb_df = convert_col_to_array(airbnb_df, element + \"_array\", element,\" \")\n",
    "  \n",
    "  #Initialize Count vectorizer\n",
    "  elementVectorizer = initialize_Count_Vectorizer(element + \"_array\",element + \"_OHE\")\n",
    "  \n",
    "  #Fit a vectorizer model\n",
    "  elementVectorizer_model = elementVectorizer.fit(airbnb_df)\n",
    "  \n",
    "  #Transform Data\n",
    "  airbnb_df = elementVectorizer_model.transform(airbnb_df)\n",
    "  \n",
    "  #Drop Extraneous Cols\n",
    "  columns_to_drop = [element, element + \"_array\"]\n",
    "  \n",
    "  #Drop columns listed above\n",
    "  airbnb_df = airbnb_df.drop(*columns_to_drop)\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "display(airbnb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the data and define some variables\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# get rid of $ sign\n",
    "df = airbnb_df\n",
    "df = df.withColumn(\"priceN\",expr(\"substring(price, 2, length(price))\"))\n",
    "df = df.drop(\"label\")\n",
    "df = df.withColumn(\"price\", df[\"priceN\"].cast(DoubleType()))\n",
    "df = df.drop(\"priceN\")\n",
    "\n",
    "# dataframe = spark.table(\"sp500_csv\") # the data table\n",
    "dataframe = df\n",
    "# cols = ['Close', 'Volume'] \n",
    "\n",
    "# The columns/features of interest\n",
    "cols = ['price','security_deposit']#,'bedrooms']\n",
    "data_array = np.array(dataframe.select(cols).toPandas())\n",
    "# data_array[:,[0, 2]] = data_array[:,[2, 0]]\n",
    "split = 0.85 # Split train and test data\n",
    "i_split = int(data_array.shape[0] * split)\n",
    "data_train = data_array[:i_split,:]\n",
    "data_test  = data_array[i_split:,:]\n",
    "len_train  = len(data_train)\n",
    "len_test   = len(data_test)\n",
    "\n",
    "seq_len = 10\n",
    "\n",
    "# Here we normalize the data to train\n",
    "import numpy as np\n",
    "def normalise_windows(window_data): \n",
    "  \n",
    "  normalised_data = []\n",
    "  for window in window_data: \n",
    "    normalised_window = []\n",
    "    for col_i in range(window.shape[1]): \n",
    "#       max_p = float(np.amax(window[:, col_i]))\n",
    "#       if max_p == 0:\n",
    "#         max_p = 1\n",
    "      for p in window[:, col_i]:\n",
    "#         if float(np.amax(window[:, col_i])) == 0:\n",
    "#           normalised_col = float(p)\n",
    "#         else:\n",
    "#         normalised_col = (float(p) / max_p)\n",
    "#         normalised_col = float(p)\n",
    "#           oldmax = float(np.amax(window[:, col_i]))\n",
    "        ii = 0\n",
    "        while ii is not (-1):\n",
    "          if not float(window[ii, col_i]) == 0:\n",
    "            normalised_col = (float(p) / float(window[ii, col_i])-1)\n",
    "            ii = -1\n",
    "          else:\n",
    "            ii +=1\n",
    "        normalised_window.append(normalised_col)\n",
    "    normalised_window = np.array(normalised_window).reshape(window.shape[0],2) # reshape and transpose array back into original multidimensional format\n",
    "    normalised_data.append(normalised_window)\n",
    "    \n",
    "  return np.array(normalised_data)\n",
    "\n",
    "\n",
    "\n",
    "# Here we define functions to get the training/test data\n",
    "\n",
    "def get_test_data(seq_len):\n",
    "  \n",
    "  data_windows = []\n",
    "  for i in range(len_test - seq_len):\n",
    "    data_windows.append(data_test[i:i+seq_len,:])\n",
    "  data_windows = np.array(data_windows).astype(float)\n",
    "  data_windows = normalise_windows(data_windows)\n",
    "  x = data_windows[:, :-1,:]\n",
    "  y = data_windows[:, -1, [0]]\n",
    "  \n",
    "  return x,y\n",
    "\n",
    "\n",
    "def get_train_data(seq_len):\n",
    "  data_x = []\n",
    "  data_y = []\n",
    "  for i in range(len_train - seq_len): \n",
    "    windowo = data_train[i:i+seq_len,:]\n",
    "    window = normalise_windows([windowo])\n",
    "    xt = window[:,:-1]\n",
    "    yt = window[:,-1, [0]]\n",
    "    data_x.append(xt)\n",
    "    data_y.append(yt)\n",
    "  data_x = np.array(data_x)[:,0,:,:]\n",
    "  data_y = np.array(data_y)[:,0,:]\n",
    "  return data_x, data_y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# functions for training data and test data and generative training\n",
    "def generate_train_batch(seq_len):#, batch_size)\n",
    "  \n",
    "#   batch_size = 128\n",
    "  i = 0\n",
    "  while i < (len_train - seq_len):\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for b in range(batch_size):\n",
    "      if i >= (len_train - seq_len):\n",
    "        # stop-condition for a smaller final batch if data doesn't divide evenly\n",
    "        \n",
    "        yield np.array(x_batch)[:,0,:,:], np.array(y_batch)[:,0,:]\n",
    "        i = 0\n",
    "      windowo = data_train[i:i+seq_len,:]\n",
    "      window = normalise_windows([windowo])\n",
    "      xt = window[:,:-1]\n",
    "      yt = window[:,-1, [0]]    \n",
    "      x_batch.append(xt)\n",
    "      y_batch.append(yt)\n",
    "      i += 1\n",
    "      \n",
    "    yield np.array(x_batch)[:,0,:,:], np.array(y_batch)[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the model and some function to call the model\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LambdaCallback\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "# Build Model\n",
    "\n",
    "neurons = 100\n",
    "dropout_rate = 0.2\n",
    "# activation =  'linear'\n",
    "activation = 'tanh'\n",
    "# activation = 'relu'\n",
    "input_timesteps = 9\n",
    "input_dim = 2\n",
    "save_dir = 'saved_models'\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "def build_model():\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(neurons,input_shape = (input_timesteps, input_dim), return_sequences=True))\n",
    "  model.add(Dropout(dropout_rate))\n",
    "  model.add(LSTM(neurons, return_sequences=True))\n",
    "  model.add(LSTM(neurons, return_sequences=False))\n",
    "  model.add(Dropout(dropout_rate))\n",
    "  model.add(Dense(1,activation = activation))\n",
    "  optimizer = Adadelta(lr=10)\n",
    "#   optimizer ='adam'\n",
    "#   optimizer ='sgd'\n",
    "\n",
    "  model.compile(loss='mse', optimizer=optimizer)\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "  \n",
    "def train_generator(data_gen):\n",
    "  steps_per_epoch = math.ceil((len_train - seq_len)/batch_size)\n",
    "  lambdacall = LambdaCallback(on_batch_begin=lambda batch,logs: print(batch))\n",
    "  callbacks = [CSVLogger(filename='save', separator=',', append=False),lambdacall]\n",
    "  model.fit_generator(data_gen,steps_per_epoch=steps_per_epoch,epochs=epochs,callbacks=callbacks,workers=0)\n",
    "\n",
    "  \n",
    "def train(x,y):#,epochs,batch_size,save_dir):\n",
    "  lambdacall = LambdaCallback(on_batch_begin=lambda batch,logs: print(batch))\n",
    "  callbacks = [CSVLogger(filename='save', separator=',', append=False),lambdacall]\n",
    "  model.fit(x,y,epochs=epochs,batch_size=batch_size,callbacks=callbacks)\n",
    "  \n",
    "def predict(data):\n",
    "  prediction_seqs = []\n",
    "  for i in range(int(len(data)/seq_len)):\n",
    "    curr_frame = data[i*seq_len]\n",
    "    predicted = []\n",
    "    for j in range(seq_len):\n",
    "      predicted.append(model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "      curr_frame = curr_frame[1:]\n",
    "      curr_frame = np.insert(curr_frame, [seq_len-2], predicted[-1], axis=0)\n",
    "    prediction_seqs.append(predicted)\n",
    "  prediction_seqs=np.array(prediction_seqs)\n",
    "  return prediction_seqs\n",
    "\n",
    "\n",
    "def eval_error(x_data,y_data):  \n",
    "  evalut = model.evaluate(x=x_data, y=y_data, batch_size=128, verbose=1)\n",
    "  return evalut\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-1224767759477153&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span><span class=\"ansi-blue-fg\">()</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">import</span> matplotlib<span class=\"ansi-blue-fg\">.</span>pyplot <span class=\"ansi-green-fg\">as</span> plt\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> seq_len <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">10</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>data_gen <span class=\"ansi-blue-fg\">=</span> generate_train_batch<span class=\"ansi-blue-fg\">(</span>seq_len<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\"># generate train data</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> train_generator<span class=\"ansi-blue-fg\">(</span>data_gen<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-red-fg\"># training</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">NameError</span>: name &#39;generate_train_batch&#39; is not defined</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run code to train the model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "seq_len = 10\n",
    "data_gen = generate_train_batch(seq_len) # generate train data\n",
    "train_generator(data_gen) # training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code to test the model\n",
    "x_test, y_test = get_test_data() # create the test data\n",
    "prediction = predict(x_test) # predict the price \n",
    "eval_error = eval_error(x_test,y_test) # evaluate the error\n",
    "print('Evaluation MSE error on Test Data = ', eval_error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "name": "NNModel",
  "notebookId": 1224767759477148
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
